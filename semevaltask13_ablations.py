# -*- coding: utf-8 -*-
"""SemEvalTask13-Ablations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R7y3Im3YBsgPgR48m8Xk9fL5-CtRZuZY
"""

!nvidia-smi

import os

BASE_DIR = "/content/semeval2026-task13"
os.makedirs(f"{BASE_DIR}/src", exist_ok=True)
os.makedirs(f"{BASE_DIR}/data", exist_ok=True)

os.listdir(BASE_DIR)

!pip install torch transformers pandas pyarrow scikit-learn tqdm

!pip install -q transformers accelerate scikit-learn

import pandas as pd

df = pd.read_parquet("/content/semeval2026-task13/data/task_a_training_set_1.parquet")
df.head()
df.columns

# BLOCK 0: installs + imports
!pip install -q transformers accelerate scikit-learn

import os
import random
import numpy as np
import pandas as pd

from dataclasses import dataclass
from typing import Dict

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from transformers import AutoTokenizer, AutoModel

# Base paths
BASE_DIR = "/content/semeval2026-task13"
DATA_DIR = f"{BASE_DIR}/data"

os.makedirs(f"{BASE_DIR}/src", exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)

print("Files in data/:", os.listdir(DATA_DIR))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# load Task A parquet and prepare train/valid

a_train_path = f"{DATA_DIR}/task_a_training_set_1.parquet"
df_a_train = pd.read_parquet(a_train_path)

print("Task A train shape:", df_a_train.shape)
print("Task A train columns:", df_a_train.columns.tolist())
display(df_a_train.head())

#  columns
TEXT_COL  = "code"
LABEL_COL = "label"

#  only needed columns
df_a_train = df_a_train[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)

# Label mapping
labels = sorted(df_a_train[LABEL_COL].unique().tolist())
label2id = {lab: i for i, lab in enumerate(labels)}
id2label = {i: lab for lab, i in label2id.items()}
num_labels = len(labels)

df_a_train["label_id"] = df_a_train[LABEL_COL].map(label2id)

print("Num labels:", num_labels)

# Train/valid split
train_df, valid_df = train_test_split(
    df_a_train,
    test_size=0.1,
    random_state=42,
    stratify=df_a_train["label_id"],
)
print("Full Train size:", len(train_df), "Full Valid size:", len(valid_df))

# split
max_train = 4000
max_valid = 1000

if len(train_df) > max_train:
    train_df = train_df.sample(n=max_train, random_state=42)

if len(valid_df) > max_valid:
    valid_df = valid_df.sample(n=max_valid, random_state=42)

train_df = train_df.reset_index(drop=True)
valid_df = valid_df.reset_index(drop=True)

print("Using subset -> Train:", len(train_df), "Valid:", len(valid_df))

# Dataset + model definition

class CodeDataset(Dataset):
    def __init__(self, df, tokenizer, max_len):
        self.texts = df[TEXT_COL].tolist()
        self.labels = df["label_id"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        enc = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt",
        )
        item = {k: v.squeeze(0) for k, v in enc.items()}
        item["labels"] = torch.tensor(label, dtype=torch.long)
        return item


class CodeBERTClassifier(nn.Module):
    def __init__(self, model_name, num_labels, dropout=0.1):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        hidden = self.encoder.config.hidden_size
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(hidden, num_labels)

    def forward(self, input_ids, attention_mask, labels=None):
        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = out.last_hidden_state[:, 0]  # CLS token
        pooled = self.dropout(pooled)
        logits = self.classifier(pooled)
        loss = None
        if labels is not None:
            loss = nn.CrossEntropyLoss()(logits, labels)
        return {"loss": loss, "logits": logits}

# training helpers + config

@dataclass
class AblationConfig:
    name: str
    model_name: str
    max_len: int = 256
    lr: float = 2e-5
    epochs: int = 1
    batch_size: int = 16
    use_onecycle: bool = True
    use_balancing: bool = True
    dropout: float = 0.1


def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def create_dataloaders(cfg: AblationConfig, tokenizer):
    train_dataset = CodeDataset(train_df, tokenizer, cfg.max_len)
    valid_dataset = CodeDataset(valid_df, tokenizer, cfg.max_len)

    # Data balancing via weighted sampler for that ablation
    if cfg.use_balancing:
        label_counts = train_df["label_id"].value_counts().to_dict()
        class_weights = {cls: 1.0 / cnt for cls, cnt in label_counts.items()}
        sample_weights = [class_weights[l] for l in train_df["label_id"]]
        sampler = WeightedRandomSampler(
            weights=sample_weights,
            num_samples=len(sample_weights),
            replacement=True,
        )
        train_loader = DataLoader(
            train_dataset,
            batch_size=cfg.batch_size,
            sampler=sampler,
        )
    else:
        train_loader = DataLoader(
            train_dataset,
            batch_size=cfg.batch_size,
            shuffle=True,
        )

    valid_loader = DataLoader(
        valid_dataset,
        batch_size=cfg.batch_size,
        shuffle=False,
    )
    return train_loader, valid_loader


def evaluate(model, dataloader):
    model.eval()
    total = 0
    correct = 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            labels = batch.pop("labels")
            out = model(**batch)
            logits = out["logits"]
            preds = torch.argmax(logits, dim=-1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            all_preds.extend(preds.cpu().tolist())
            all_labels.extend(labels.cpu().tolist())

    acc = correct / total
    f1_macro = f1_score(all_labels, all_preds, average="macro")
    return acc, f1_macro


def train_and_eval(cfg: AblationConfig) -> Dict:
    print(f"\n===== Experiment: {cfg.name} =====")
    set_seed(42)
    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)
    train_loader, valid_loader = create_dataloaders(cfg, tokenizer)

    model = CodeBERTClassifier(
        model_name=cfg.model_name,
        num_labels=num_labels,
        dropout=cfg.dropout,
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)

    if cfg.use_onecycle:
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=cfg.lr,
            steps_per_epoch=len(train_loader),
            epochs=cfg.epochs,
        )
    else:
        scheduler = None  # constant LR

    best_acc = 0.0
    best_f1 = 0.0

    for epoch in range(cfg.epochs):
        model.train()
        total_loss = 0.0
        for step, batch in enumerate(train_loader):
            batch = {k: v.to(device) for k, v in batch.items()}
            labels = batch.pop("labels")

            optimizer.zero_grad()
            out = model(**batch, labels=labels)
            loss = out["loss"]
            loss.backward()
            optimizer.step()
            if scheduler is not None:
                scheduler.step()

            total_loss += loss.item()

            if step % 100 == 0:
                print(f"  step {step}/{len(train_loader)}", end="\r")

        avg_loss = total_loss / max(1, len(train_loader))
        val_acc, val_f1 = evaluate(model, valid_loader)
        best_acc = max(best_acc, val_acc)
        best_f1 = max(best_f1, val_f1)

        print(
            f"Epoch {epoch+1}/{cfg.epochs} | "
            f"Loss {avg_loss:.4f} | "
            f"Val Acc {val_acc:.4f} | Val F1 {val_f1:.4f}"
        )

    return {
        "experiment": cfg.name,
        "encoder": cfg.model_name,
        "max_len": cfg.max_len,
        "use_onecycle": cfg.use_onecycle,
        "use_balancing": cfg.use_balancing,
        "best_val_acc": best_acc,
        "best_val_f1": best_f1,
    }

# define ablation experiments

# Baseline encoder
CODEBERT = "microsoft/codebert-base"
# Distilled encoder proxy for DistilCodeBERT
DISTIL_CODEBERT = "huggingface/CodeBERTa-small-v1"

EXPERIMENTS = [
    # codeBERT, max_len=256, OneCycle, balanced
    AblationConfig(
        name="baseline_codebert_256_onecycle_balanced",
        model_name=CODEBERT,
        max_len=256,
        use_onecycle=True,
        use_balancing=True,
    ),
    # Encoder variants- Distilled encoder vs CodeBERT
    AblationConfig(
        name="distilcodeberta_256_onecycle_balanced",
        model_name=DISTIL_CODEBERT,
        max_len=256,
        use_onecycle=True,
        use_balancing=True,
    ),
    # Input length - 128 tokens vs 256
    AblationConfig(
        name="codebert_128_onecycle_balanced",
        model_name=CODEBERT,
        max_len=128,
        use_onecycle=True,
        use_balancing=True,
    ),
    # Training strategy- constant LR vs OneCycle
    AblationConfig(
        name="codebert_256_constantLR_balanced",
        model_name=CODEBERT,
        max_len=256,
        use_onecycle=False,   # constant LR
        use_balancing=True,
    ),
    # Data balancing- no sampler vs sampler
    AblationConfig(
        name="codebert_256_onecycle_unbalanced",
        model_name=CODEBERT,
        max_len=256,
        use_onecycle=True,
        use_balancing=False,  # no WeightedRandomSampler
    ),
]

# running all ablation experiments

all_results = []
for cfg in EXPERIMENTS:
    res = train_and_eval(cfg)
    all_results.append(res)

results_df = pd.DataFrame(all_results)
print("\nAblation summary:")
display(results_df)

# save to CSV
csv_path = f"{BASE_DIR}/ablation_taskA.csv"
results_df.to_csv(csv_path, index=False)
print("Saved ablation results to:", csv_path)

# load Task B parquet and prepare train/valid

b_train_path = f"{DATA_DIR}/task_b_training_set.parquet"
df_b_train = pd.read_parquet(b_train_path)

print("Task B train shape:", df_b_train.shape)
print("Task B train columns:", df_b_train.columns.tolist())
display(df_b_train.head())

# Assuming same columns
TEXT_COL  = "code"
LABEL_COL = "label"

# only needed columns
df_b_train = df_b_train[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)

# Label mapping
labels = sorted(df_b_train[LABEL_COL].unique().tolist())
label2id = {lab: i for i, lab in enumerate(labels)}
id2label = {i: lab for lab, i in label2id.items()}
num_labels = len(labels)

df_b_train["label_id"] = df_b_train[LABEL_COL].map(label2id)

print("Num labels (Task B):", num_labels)

# Train/valid split
train_df, valid_df = train_test_split(
    df_b_train,
    test_size=0.1,
    random_state=42,
    stratify=df_b_train["label_id"],
)
print("Full Train size (B):", len(train_df), "Full Valid size (B):", len(valid_df))

max_train = 4000
max_valid = 1000

if len(train_df) > max_train:
    train_df = train_df.sample(n=max_train, random_state=42)

if len(valid_df) > max_valid:
    valid_df = valid_df.sample(n=max_valid, random_state=42)

train_df = train_df.reset_index(drop=True)
valid_df = valid_df.reset_index(drop=True)

print("Using subset (Task B) -> Train:", len(train_df), "Valid:", len(valid_df))

# run all ablation experiments for Task B

all_results_b = []
for cfg in EXPERIMENTS:
    res = train_and_eval(cfg)
    all_results_b.append(res)

results_b_df = pd.DataFrame(all_results_b)
print("\nTask B Ablation summary:")
display(results_b_df)

csv_path_b = f"{BASE_DIR}/ablation_taskB.csv"
results_b_df.to_csv(csv_path_b, index=False)
print("Saved Task B ablation results to:", csv_path_b)

# load Task C parquet and prepare train/valid

c_train_path = f"{DATA_DIR}/task_c_training_set_1.parquet"
df_c_train = pd.read_parquet(c_train_path)

print("Task C train shape:", df_c_train.shape)
print("Task C train columns:", df_c_train.columns.tolist())
display(df_c_train.head())

TEXT_COL  = "code"
LABEL_COL = "label"

# only needed columns
df_c_train = df_c_train[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)

# Label mapping
labels = sorted(df_c_train[LABEL_COL].unique().tolist())
label2id = {lab: i for i, lab in enumerate(labels)}
id2label = {i: lab for lab, i in label2id.items()}
num_labels = len(labels)

df_c_train["label_id"] = df_c_train[LABEL_COL].map(label2id)

print("Num labels (Task C):", num_labels)

# Train/valid split
train_df, valid_df = train_test_split(
    df_c_train,
    test_size=0.1,
    random_state=42,
    stratify=df_c_train["label_id"],
)
print("Full Train size (C):", len(train_df), "Full Valid size (C):", len(valid_df))


max_train = 4000
max_valid = 1000

if len(train_df) > max_train:
    train_df = train_df.sample(n=max_train, random_state=42)

if len(valid_df) > max_valid:
    valid_df = valid_df.sample(n=max_valid, random_state=42)

train_df = train_df.reset_index(drop=True)
valid_df = valid_df.reset_index(drop=True)

print("Using subset (Task C) -> Train:", len(train_df), "Valid:", len(valid_df))

# running all ablation experiments for Task C

all_results_c = []
for cfg in EXPERIMENTS:
    res = train_and_eval(cfg)
    all_results_c.append(res)

results_c_df = pd.DataFrame(all_results_c)
print("\nTask C Ablation summary:")
display(results_c_df)

csv_path_c = f"{BASE_DIR}/ablation_taskC.csv"
results_c_df.to_csv(csv_path_c, index=False)
print("Saved Task C ablation results to:", csv_path_c)

